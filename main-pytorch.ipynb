{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac11d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import torch to create dataloader used later in project\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# autoloads dependancies when edits are made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c802019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "CHANNEL_NUM = 3\n",
    "\n",
    "IMAGE_DIMS = (IMAGE_HEIGHT, IMAGE_WIDTH, CHANNEL_NUM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea611412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# HARDWARE SETUP\n",
    "\n",
    "# Checks CUDA to see if it is operational\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "PIN_MEMORY = True\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46f0a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4315 images in the landscape directory\n",
      "There are 300 images in the abstract directory\n"
     ]
    }
   ],
   "source": [
    "# Directory\n",
    "\n",
    "# reading the number of images in each domain\n",
    "\n",
    "import os, os.path\n",
    "\n",
    "IMG_DIR = os.getcwd() +\"\\\\images\"\n",
    "\n",
    "LANDSCAPE_DIR = IMG_DIR + \"\\\\landscape\"\n",
    "\n",
    "ABSTRACT_DIR = IMG_DIR + \"\\\\monet_jpg\"\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "for name in os.listdir(LANDSCAPE_DIR):\n",
    "    directory = LANDSCAPE_DIR + \"\\\\\" + name\n",
    "    image = Image.open(directory)\n",
    "    if image.mode != \"RGB\":\n",
    "        print(\"Incompatible data type!\")\n",
    "        \n",
    "def get_number_of_images(DIR):\n",
    "    return len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
    "\n",
    "LANDSCAPE_IMAGE_COUNT = get_number_of_images(LANDSCAPE_DIR)\n",
    "ABSTRACT_IMAGE_COUNT = get_number_of_images(ABSTRACT_DIR)\n",
    "\n",
    "print(f\"There are {LANDSCAPE_IMAGE_COUNT} images in the landscape directory\")\n",
    "print(f\"There are {ABSTRACT_IMAGE_COUNT} images in the abstract directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6b78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we instantiate the loaders that will prepare our images for training\n",
    "\n",
    "from dataloader import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_loaders(\n",
    "        landscape_dir,\n",
    "        abstract_dir,\n",
    "        batch_size,\n",
    "        landscape_transform,\n",
    "        abstract_transform,\n",
    "        num_workers,\n",
    "        pin_memory,\n",
    "        landscape_shuffle,\n",
    "        abstract_shuffle\n",
    "    ):\n",
    "    \n",
    "    landscape_ds = ImageDataset(image_dir = landscape_dir, transform=landscape_transform\n",
    "    )\n",
    "    \n",
    "    landscape_loader = DataLoader(\n",
    "        landscape_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=landscape_shuffle\n",
    "    )\n",
    "    \n",
    "    abstract_ds = ImageDataset(image_dir=abstract_dir, transform=abstract_transform\n",
    "    )\n",
    "    \n",
    "    abstract_loader = DataLoader(\n",
    "        abstract_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=abstract_shuffle\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return landscape_loader, abstract_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7328dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAINING_VALIDATION_TESTING SETS\n",
    "\n",
    "# TRAINING SET IS 70%\n",
    "# VALIDATION SET IS 20%\n",
    "# TESTING SET IS 10%\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_size = int(LANDSCAPE_IMAGE_COUNT * 0.7)\n",
    "validation_size=  int(LANDSCAPE_IMAGE_COUNT * 0.2)\n",
    "testing_size = LANDSCAPE_IMAGE_COUNT - train_size - validation_size\n",
    "\n",
    "assert LANDSCAPE_IMAGE_COUNT == (train_size + validation_size + testing_size), \"unequal training_validation_test partitions\"\n",
    "\n",
    "landscape_transform = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(scale_limit=0.20, rotate_limit=10, shift_limit=0.1, p=0.5, border_mode=cv2.BORDER_REFLECT),\n",
    "                A.GridDistortion(p=0.5),\n",
    "                A.Normalize(\n",
    "                    mean=[0],\n",
    "                    std=[1],\n",
    "                    \n",
    "                ),\n",
    "                A.Resize(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                ToTensorV2(),\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "abstract_transform = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(scale_limit=0.20, rotate_limit=10, shift_limit=0.1, p=0.5, border_mode=cv2.BORDER_REFLECT),\n",
    "                A.GridDistortion(p=0.5),\n",
    "                A.Normalize(\n",
    "                    mean=[0],\n",
    "                    std=[1],\n",
    "                    \n",
    "                ),\n",
    "                A.Resize(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                ToTensorV2(),\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "landscape_loader, abstract_loader = get_loaders(\n",
    "                    LANDSCAPE_DIR,\n",
    "                    ABSTRACT_DIR,\n",
    "                    BATCH_SIZE,\n",
    "                    landscape_transform,\n",
    "                    abstract_transform,\n",
    "                    NUM_WORKERS,\n",
    "                    PIN_MEMORY,\n",
    "                    True,\n",
    "                    True\n",
    "                        \n",
    "                )\n",
    "\n",
    "# ls_training_loader, validation_loader, testing_loader = torch.utils.data.random_split(landscape_loader.dataset, [train_size, validation_size, testing_size])\n",
    "# train_dataset, val_dataset = get_loaders()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cb4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in ls_training_loader:\n",
    "#     pass\n",
    "# #     plt.imshow(data, interpolation='nearest')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eacf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "# from matplotlib import cm\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def demo_image_from_loader(loader):\n",
    "    for idx, x in enumerate(loader):\n",
    "        x = x.squeeze(0)\n",
    "        print(x.shape)\n",
    "        img = ToPILImage()(x)\n",
    "        img.show()\n",
    "        break\n",
    "        \n",
    "def demo_image(tensor):\n",
    "    img = ToPILImage()(tensor)\n",
    "    img.show()\n",
    "    \n",
    "    \n",
    "# demo_image_from_loader(landscape_loader)\n",
    "# demo_image_from_loader(abstract_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561a25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ai-art-translation\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Input\n",
    "\n",
    "from utils import generate_real_samples, generate_fake_samples, update_image_pool\n",
    "# from models import Discriminator\n",
    "\n",
    "# we create our discriminator and generator networks next,\n",
    "\n",
    "\n",
    "from models import *\n",
    "\n",
    "from dataloader import *\n",
    "\n",
    "# d_model_ls = define_discriminator(IMAGE_DIMS, 64)\n",
    "# d_model_art = define_discriminator(IMAGE_DIMS, 64)\n",
    "\n",
    "# g_model_ls = define_generator(IMAGE_DIMS, blocks=6, f=64)\n",
    "# g_model_art = define_generator(IMAGE_DIMS, blocks=6, f=64)\n",
    "\n",
    "# img_l, targets_l = generate_real_samples(landscape_loader, 5)\n",
    "# img_a, targets_a = generate_real_samples(abstract_loader, 5)\n",
    "\n",
    "# print(img_l.shape)\n",
    "# print(img_a.shape)\n",
    "\n",
    "# img_l_f, targets_l_f = generate_fake_samples(g_model_ls, img_l)\n",
    "\n",
    "# preds_l = g_model_ls(img_l)\n",
    "# preds_a = g_model_art(img_a)\n",
    "\n",
    "\n",
    "\n",
    "# print(preds_l.shape)\n",
    "# print(preds_l.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2852a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "    \n",
    "\n",
    "# print(landscape_loader[1].shape)\n",
    "# pred=model(landscape_loader[1].unsqueeze(0))\n",
    "\n",
    "# print(torch.argmax(pred))\n",
    "# print(pred)\n",
    "# print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dda5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img, targets = generate_real_samples(landscape_loader, 5)\n",
    "\n",
    "# print(img.shape)\n",
    "# print(targets)\n",
    "\n",
    "# fake_img, fake_targets = generate_fake_samples(g_model_ls, img)\n",
    "\n",
    "\n",
    "\n",
    "# print(img.shape)\n",
    "# print(targets)\n",
    "\n",
    "# print(fake_img.shape)\n",
    "# print(fake_targets)\n",
    "\n",
    "# X_realLs, y_realLs = generate_real_samples(landscape_loader, 5)\n",
    "# X_realArt, y_realArt = generate_real_samples(abstract_loader, 5)\n",
    "        \n",
    "        \n",
    "# pool = update_image_pool([], fake_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52e8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_fn(loader, model, optimizer, loss_fn, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67e416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 142 calls to <function Model.make_train_function.<locals>.train_function at 0x0000025B1EC81DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Step 1 of 4315 completed!\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      ">Saved: g_model_LstoArt_000002.h5 and g_model_ArttoLs_000003.h5\n",
      "Step 101 of 4315 completed!\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      ">Saved: g_model_LstoArt_000102.h5 and g_model_ArttoLs_000103.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6292/3546922389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m train(d_model_ls, d_model_art, g_model_ls_to_art\n\u001b[0m\u001b[0;32m    120\u001b[0m       \u001b[1;33m,\u001b[0m \u001b[0mg_model_art_to_ls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_model_ls_to_art\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m       , c_model_art_to_ls, landscape_loader, abstract_loader)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6292/3546922389.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(d_model_Ls, d_model_Art, g_model_LstoArt, g_model_ArttoLs, c_model_LstoArt, c_model_ArttoLs, landscape_loader, abstract_loader)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mdLs_loss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model_Ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_realLs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_realLs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mdLs_loss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model_Ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_fakeLs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fakeLs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         g_loss1, _, _, _, _ = c_model_LstoArt.train_on_batch([X_realLs, X_realArt], [y_realArt,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1850\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0m\u001b[0;32m   1853\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                                                     class_weight)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3119\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3120\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The ideal size is Batch, Height, Width, Channels\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def save_models(step, g_model_LstoArt, g_model_ArttoLs):   \n",
    "    filename1 = 'g_model_LstoArt_%06d.h5' % (step+1)\n",
    "    \n",
    "    filename2 = 'g_model_ArttoLs_%06d.h5' % (step+2)\n",
    "    \n",
    "    g_model_LstoArt.save(filename1)\n",
    "    g_model_ArttoLs.save(filename2)\n",
    "    \n",
    "    print('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n",
    "    \n",
    "    g_model_1.trainable = True\n",
    "    \n",
    "    d_model.trainable = False\n",
    "    \n",
    "    g_model_2.trainable = False\n",
    "    \n",
    "    # two composite models relate to steps to improve each generator\n",
    "    \n",
    "    ## TODO ##\n",
    "    \n",
    "    # get image from dataloader, I can define if it is real or not later\n",
    "    #\n",
    "\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "    \n",
    "    # here we take an authentic image and assess if the output of a generator is real or fake\n",
    "    ##########\n",
    "    \n",
    "    # identity element\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "    \n",
    "    # identity function should get nothing occur to it, as it is Going from A -> A\n",
    "    \n",
    "    # forward cycle\n",
    "    output_f = g_model_2(gen1_out)\n",
    "    \n",
    "    # this can be compared to input_img to determine if details of object is retained.\n",
    "    \n",
    "    # backward cycle\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "    \n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "    \n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    \n",
    "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1,5,10,10], optimizer=opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(d_model_Ls, d_model_Art, g_model_LstoArt, g_model_ArttoLs, c_model_LstoArt, c_model_ArttoLs, landscape_loader, abstract_loader):\n",
    "    n_epochs, n_batch = NUM_EPOCHS, BATCH_SIZE\n",
    "    \n",
    "    \n",
    "#     poolLs, poolArt = list(), list()\n",
    "    \n",
    "    bat_per_epo = int(LANDSCAPE_IMAGE_COUNT/ n_batch)\n",
    "    \n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # selecting batches of real samples\n",
    "        X_realLs, y_realLs = generate_real_samples(landscape_loader, n_batch)\n",
    "        X_realArt, y_realArt = generate_real_samples(abstract_loader, n_batch)\n",
    "        \n",
    "        # generate a batch of fake samples\n",
    "        X_fakeLs, y_fakeLs = generate_fake_samples(g_model_ArttoLs, X_realArt)\n",
    "        X_fakeArt, y_fakeArt = generate_fake_samples(g_model_LstoArt, X_realLs)\n",
    "        # update fakes from pool\n",
    "        \n",
    "#         X_fakeLs = update_image_pool(poolLs, X_fakeLs)\n",
    "#         X_fakeArt = update_image_pool(PoolArt, X_fakeArt)\n",
    "        \n",
    "        g_loss2, _, _, _, _ = c_model_ArttoLs.train_on_batch([X_realArt, X_realLs], [y_realLs,\n",
    "                                                                                    X_realLs,\n",
    "                                                                                    X_realArt,\n",
    "                                                                                    X_realLs])\n",
    "        \n",
    "        dLs_loss1 = d_model_Ls.train_on_batch(X_realLs, y_realLs)\n",
    "        dLs_loss2 = d_model_Ls.train_on_batch(X_fakeLs, y_fakeLs)\n",
    "        \n",
    "        g_loss1, _, _, _, _ = c_model_LstoArt.train_on_batch([X_realLs, X_realArt], [y_realArt,\n",
    "                                                                                    X_realArt,\n",
    "                                                                                    X_realLs,\n",
    "                                                                                    X_realArt])\n",
    "        dArt_loss1 = d_model_Art.train_on_batch(X_realArt, y_realArt)\n",
    "        dArt_loss2 = d_model_Art.train_on_batch(X_fakeArt, y_fakeArt)\n",
    "        \n",
    "        if i % (100) == 0:\n",
    "            print(\"Step {} of {} completed!\".format((i+1), n_steps))\n",
    "            save_models(i+1, g_model_LstoArt, g_model_ArttoLs)\n",
    "\n",
    "\n",
    "    \n",
    "image_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
    "\n",
    "g_model_ls_to_art = define_generator(image_shape, blocks = 6, f=64)\n",
    "g_model_art_to_ls = define_generator(image_shape, blocks = 6, f=64)\n",
    "# g_model_ls_to_art.summary()\n",
    "\n",
    "d_model_ls = define_discriminator(image_shape, filters=64)\n",
    "d_model_art = define_discriminator(image_shape, filters=64)\n",
    "\n",
    "c_model_ls_to_art = define_composite_model(g_model_ls_to_art, d_model_art, g_model_art_to_ls, image_shape)\n",
    "c_model_art_to_ls = define_composite_model(g_model_art_to_ls, d_model_ls, g_model_ls_to_art, image_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train(d_model_ls, d_model_art, g_model_ls_to_art\n",
    "      , g_model_art_to_ls, c_model_ls_to_art\n",
    "      , c_model_art_to_ls, landscape_loader, abstract_loader)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae62d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "    X_in, _ = generate_real_samples(trainX, n_samples)\n",
    "    \n",
    "    X_out, _ = generate_fake_samples(g_model, X_in)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0afabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, targets = generate_real_samples(landscape_loader, 5)\n",
    "\n",
    "fake_img, fake_targets = generate_fake_samples(g_model_ls, img)\n",
    "\n",
    "print(img.shape)\n",
    "print(targets)\n",
    "\n",
    "print(fake_img.shape)\n",
    "print(fake_targets)\n",
    "\n",
    "pool = update_image_pool([], fake_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94840b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps are to creates batches of real sample functions, from domain ls and art, and their respective labels (Smooth labelling?)\n",
    "\n",
    "# then i do the same but for fake data (structural change)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98534d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209690f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-art-translation",
   "language": "python",
   "name": "ai-art-translation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
